# ðŸ§ª README: MCQA vs Free-Form Answer Matching Evaluator

This project provides a simple, extensible framework to evaluate language models using both:

- âœ… **Multiple-Choice Question Answering (MCQA)**
- ðŸ¤– **Free-form Answer Matching** (Fuzzy and Semantic)

---

## ðŸ“‚ File Structure

```bash
â”œâ”€â”€ mcqa_vs_freeform_eval.py  # Core script (MCQA + Answer Matching logic)
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ README.md                 # Project overview and usage
```

---

## ðŸš€ Features

- Compare MCQA accuracy with flexible open-answer evaluations
- Fuzzy string matching using RapidFuzz
- Semantic similarity using Sentence Transformers (`all-MiniLM-L6-v2`)
- CLI/Notebook-style evaluation runner

---

## ðŸ“¦ Installation

```bash
pip install -r requirements.txt
```

Dependencies:

```
rapidfuzz
sentence-transformers
torch
```

---

## ðŸ§ª How to Run

```bash
python mcqa_vs_freeform_eval.py
```

Or copy the script into a Jupyter notebook cell to test interactively.

---

## ðŸ“ Example Output

```
=== MCQA vs Answer Matching Evaluation ===

ðŸ§ª MCQA Accuracy: 66.67%
ðŸ§ª Fuzzy Match Accuracy: 66.67%
ðŸ§  Semantic Match Accuracy: 100.00%
```

---

## ðŸ“Š Use Cases

- Evaluate generative models that don't produce labels
- Compare classical vs free-form QA benchmarks
- Test reasoning quality beyond forced choice

---

## ðŸ”§ Extending the Project

### Add Your Own Questions

In `mcqa_vs_freeform_eval.py`, edit:

```python
questions = [ ... ]
choices = [ ... ]
correct_mcqa = [ ... ]
model_mcqa = [ ... ]
reference_answers = [ ... ]
model_answers = [ ... ]
```

### Add a Web UI (Optional)

To create a live demo:

```bash
pip install streamlit
```

Then create `app.py` with Streamlit or use Gradio.

Would you like us to generate a Streamlit or Gradio app next?

---

## ðŸ§  Citation

Based on the evaluation techniques discussed in:

> "Answer Matching Outperforms Multiple Choice for Language Model Evaluation" â€” arXiv:2507.02856

